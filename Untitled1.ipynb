{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31fc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from csaps import csaps\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def hampel_filter_forloop(input_series, window_size=15, n_sigmas=4):\n",
    "    \n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    # possibly use np.nanmedian \n",
    "    for i in range((window_size),(n - window_size)):\n",
    "        x0 = np.median(input_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.median(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
    "            new_series[i] = x0\n",
    "            indices.append(i)\n",
    "    \n",
    "    return new_series, indices\n",
    "\n",
    "\n",
    "def Smooth_epiData(data_4,smooth_factor = 14,week_correction = 1,week_smoothing = 1):\n",
    "    \n",
    "    if smooth_factor<=0:\n",
    "        return data_4\n",
    "\n",
    "    data_4 = pd.DataFrame(data_4).interpolate(method='linear',limit=2,limit_direction='both')\n",
    "\n",
    "    deldata = np.diff(data_4, axis=1)\n",
    "\n",
    "    deldata = pd.DataFrame(deldata).T\n",
    "\n",
    "    data_4_s = data_4\n",
    "\n",
    "    maxt = data_4.shape[1]\n",
    "\n",
    "    date_map = np.ceil((np.arange(1, maxt) - np.mod(maxt-1, 7))/7).reshape(1,-1)\n",
    "\n",
    "    cleandel = deldata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if week_correction==1:\n",
    "        for cid in range(len(data_4)): \n",
    "            week_dat = pd.DataFrame(data_4).loc[cid][list(range((maxt-1)%7,maxt,7))].diff()[1:]\n",
    "\n",
    "            from scipy import signal\n",
    "\n",
    "\n",
    "\n",
    "            clean_week_tmp,TF = hampel_filter_forloop(week_dat.to_numpy())\n",
    "\n",
    "            week_dat_tmp = week_dat.to_list()\n",
    "\n",
    "            for idx_out in TF:\n",
    "                week_dat_tmp[idx_out] = np.nan\n",
    "\n",
    "            clean_week = pd.DataFrame(week_dat_tmp).interpolate(method='linear')\n",
    "\n",
    "            week_dat_1 = week_dat.to_list()\n",
    "            week_dat_1.append(0)\n",
    "            peak_idx = np.array([i for i in range(1, len(week_dat_1)-1) if week_dat_1[i] > week_dat_1[i-1] and week_dat_1[i] > week_dat_1[i+1]])\n",
    "\n",
    "\n",
    "            tf_vals = []\n",
    "            for i in TF:\n",
    "                if i in peak_idx:\n",
    "            #         print(i)\n",
    "                    tf_vals.append(i)\n",
    "\n",
    "        #     cid = 0\n",
    "\n",
    "            for jj in tf_vals:\n",
    "        #         print(jj)\n",
    "                get_idx = [w for w in range(len(date_map[0])) if date_map[0][w]==jj]\n",
    "        #         print(get_idx)\n",
    "                for w in get_idx:\n",
    "        #             print(cleandel.iloc[w][cid])\n",
    "                    cleandel.iloc[w][cid] = clean_week.iloc[jj][0]/7\n",
    "        #             print(cleandel.iloc[w][cid])\n",
    "\n",
    "        deldata = cleandel\n",
    "\n",
    "#     week_smoothing = 1\n",
    "\n",
    "    # if week_smoothing == 1:\n",
    "\n",
    "    if week_smoothing == 1:\n",
    "        temp = np.cumsum(deldata.T, axis=1)\n",
    "\n",
    "        temp['new'] = 0 ### add a constant colum with value 0 \n",
    "\n",
    "        temp = temp[[list(temp.columns)[-1]]+list(temp.columns)[0:-1]]\n",
    "\n",
    "        temp_array = np.array(temp)\n",
    "\n",
    "        diff = np.diff(temp_array[:,(maxt-1)%7::7].T,axis=0)\n",
    "\n",
    "        week_dat = pd.DataFrame(diff)\n",
    "\n",
    "        xx = np.full((deldata.shape), np.nan)\n",
    "\n",
    "        xx[int((maxt-1-7*week_dat.shape[0])+7)-1:maxt-1:7,:] = np.cumsum(week_dat, axis=0)\n",
    "\n",
    "        xx = pd.DataFrame(xx).interpolate(method='linear',limit_direction='both')\n",
    "\n",
    "        deldata.iloc[1:,:] = np.diff(xx, axis=0)\n",
    "\n",
    "    deldata[deldata<0] = 0\n",
    "\n",
    "    for state_idx in range(deldata.shape[1]):\n",
    "\n",
    "        deldata[state_idx] = deldata[state_idx].rolling(smooth_factor,min_periods=0).mean()\n",
    "\n",
    "\n",
    "    data_4_s = np.concatenate((data_4.iloc[:,:1].to_numpy(),np.cumsum(deldata,axis=0).T),axis=1)\n",
    "\n",
    "    return data_4_s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d32fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def CDC_SERO_Function():\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Load sero data from CDC\n",
    "url = 'https://data.cdc.gov/api/views/mtc3-kq6r/rows.csv?accessType=DOWNLOAD'\n",
    "response = requests.get(url)\n",
    "open('dummy.csv', 'wb').write(response.content)\n",
    "sero_data = pd.read_csv('dummy.csv')\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"data_4.pkl\", 'rb') \n",
    "#     print(a)\n",
    "data_4 = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "sero_data['Region Abbreviation_tmp'] = sero_data['Region Abbreviation'].str[:2]\n",
    "\n",
    "sero_data['x1_tmp'] = sero_data[\"Lower CI %[Total Prevalence]\"]*sero_data[\"n [Total Prevalence]\"]\n",
    "\n",
    "sero_data['x2_tmp'] = sero_data[\"Rate %[Total Prevalence]\"]*sero_data[\"n [Total Prevalence]\"]\n",
    "\n",
    "sero_data['x3_tmp'] = sero_data[\"Upper CI  %[Total Prevalence]\"]*sero_data[\"n [Total Prevalence]\"]\n",
    "\n",
    "tmp = sero_data.groupby(['Region Abbreviation_tmp','Median \\nDonation Date']).agg({\"n [Total Prevalence]\":\"sum\",\n",
    "                                                   }).reset_index()\n",
    "\n",
    "sero_subset = sero_data[['Region Abbreviation_tmp','Median \\nDonation Date','x1_tmp','x2_tmp','x3_tmp']]\n",
    "\n",
    "# sero_subset['key'] = sero_subset['Region Abbreviation_tmp']+\"_\"+sero_subset['Median \\nDonation Date']\n",
    "\n",
    "new_df = pd.merge(sero_subset, tmp,  how='left', left_on=['Region Abbreviation_tmp','Median \\nDonation Date'],\\\n",
    "                  right_on = ['Region Abbreviation_tmp','Median \\nDonation Date'])\n",
    "\n",
    "new_df['x1'] = (new_df['x1_tmp']/new_df['n [Total Prevalence]'])*0.01\n",
    "new_df['x2'] = (new_df['x2_tmp']/new_df['n [Total Prevalence]'])*0.01\n",
    "new_df['x3'] = (new_df['x3_tmp']/new_df['n [Total Prevalence]'])*0.01\n",
    "\n",
    "\n",
    "sero_data_processed = new_df.groupby(['Region Abbreviation_tmp','Median \\nDonation Date']).agg({\"x1\":\"sum\",\\\n",
    "                                                                                                \"x2\":\"sum\",\\\n",
    "                                                                                                \"x3\":\"sum\",\\\n",
    "                                                   }).reset_index()\n",
    "\n",
    "\n",
    "abvs = pd.read_csv('us_states_abbr_list.txt', header=None)\n",
    "\n",
    "state_map = dict(zip(abvs[0], range(len(abvs))))\n",
    "try:\n",
    "    xx = sero_data_processed['Median \\nDonation Date']\n",
    "except:\n",
    "    xx = sero_data_processed['MedianDonationDate']\n",
    "whichday = [0]*len(xx)\n",
    "\n",
    "for ii in range(len(xx)):\n",
    "    whichday[ii] = (datetime.strptime(xx[ii], '%m/%d/%Y') - datetime(2020, 1, 23)).days\n",
    "\n",
    "\n",
    "\n",
    "un_ts = np.empty((data_4.shape[0],data_4.shape[1]))*np.nan\n",
    "un_lts = np.empty((data_4.shape[0],data_4.shape[1]))*np.nan\n",
    "un_uts = np.empty((data_4.shape[0],data_4.shape[1]))*np.nan\n",
    "\n",
    "sero_data_processed['idx'] = sero_data_processed['Median \\nDonation Date'].apply(lambda x :\\\n",
    "                                    (datetime.strptime(x, '%m/%d/%Y') - datetime(2020, 1, 23)).days\n",
    "                                                                                )\n",
    "\n",
    "\n",
    "sero_data_processed['cidx'] = sero_data_processed['Region Abbreviation_tmp'].apply(lambda x : \\\n",
    "                                                   state_map.get(x,None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa0ce241",
   "metadata": {},
   "outputs": [],
   "source": [
    "popu_tmp = pd.read_csv(\"us_states_population_data.txt\",header=None)\n",
    "\n",
    "popu = popu_tmp[0].to_list()\n",
    "\n",
    "for cidx in range(data_4.shape[0]):\n",
    "    for ii in range(data_4.shape[1]):\n",
    "#         print(cidx)\n",
    "        if len(sero_data_processed[sero_data_processed['cidx'] == cidx])>0:\n",
    "            if len(sero_data_processed[sero_data_processed['idx'] == ii])>0:\n",
    "                data_subset = sero_data_processed[(sero_data_processed['cidx']==cidx) & (sero_data_processed['idx']==ii)]\n",
    "                if len(data_subset)>0:\n",
    "                    x1 = data_subset['x1'].values[0]\n",
    "                    x2 = data_subset['x2'].values[0]\n",
    "                    x3 = data_subset['x3'].values[0]                \n",
    "    #                     print(x1*popu[cidx])\n",
    "    #                 x2 = \n",
    "#                     try:\n",
    "#                         data_4[cidx, ii]\n",
    "#                         print(\"aaa\",cidx,ii)\n",
    "\n",
    "#                     except:\n",
    "#                         print(cidx,ii)\n",
    "#                         break\n",
    "#                     try:\n",
    "                    un_lts[cidx, (ii)] = x1*popu[cidx]/data_4[cidx, ii]\n",
    "                    un_ts[cidx, (ii)] = x2*popu[cidx]/data_4[cidx, ii]\n",
    "                    un_uts[cidx, (ii)] = x3*popu[cidx]/data_4[cidx, ii]\n",
    "#                     except:\n",
    "#                         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56eb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c731630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "thisday = data_4.shape[1]\n",
    "\n",
    "un_ts[un_ts<1] = np.nan\n",
    "un_ts_s = pd.DataFrame(un_ts).interpolate(axis=1,limit_direction='both').fillna(1).to_numpy()\n",
    "\n",
    "\n",
    "un_lts[un_lts<1] = np.nan\n",
    "un_lts_s = pd.DataFrame(un_lts).interpolate(axis=1,limit_direction='both').fillna(1).to_numpy()\n",
    "\n",
    "un_uts[un_uts<1] = np.nan\n",
    "un_uts_s = pd.DataFrame(un_uts).interpolate(axis=1,limit_direction='both').fillna(1).to_numpy()\n",
    "\n",
    "\n",
    "dd_s = Smooth_epiData(data_4)\n",
    "ddata = np.diff(dd_s)\n",
    "\n",
    "\n",
    "temp = pd.DataFrame(ddata)\n",
    "temp['new'] = 0 ### add a constant colum with value 0 \n",
    "\n",
    "ddata = temp[[list(temp.columns)[-1]]+list(temp.columns)[0:-1]]\n",
    "\n",
    "\n",
    "\n",
    "true_new_infec = [\n",
    "    pd.DataFrame(np.multiply(un_lts_s,ddata.to_numpy())).rolling(window=28,axis=1,min_periods=0).mean(),\n",
    "    pd.DataFrame(np.multiply(un_ts_s,ddata.to_numpy())).rolling(window=28,axis=1,min_periods=0).mean(),\n",
    "    pd.DataFrame(np.multiply(un_uts_s,ddata.to_numpy())).rolling(window=28,axis=1,min_periods=0).mean()\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "un_array = [\n",
    "    np.sum(np.multiply(un_lts_s,ddata.to_numpy()),axis=1)/dd_s[:,-1],\n",
    "    np.sum(np.multiply(un_ts_s,ddata.to_numpy()),axis=1)/dd_s[:,-1],\n",
    "    np.sum(np.multiply(un_uts_s,ddata.to_numpy()),axis=1)/dd_s[:,-1]\n",
    "]\n",
    "\n",
    "un_array[0][np.isnan(un_array[0])] = 1\n",
    "\n",
    "un_array[1][np.isnan(un_array[1])] = 1\n",
    "\n",
    "un_array[2][np.isnan(un_array[2])] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22764ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
