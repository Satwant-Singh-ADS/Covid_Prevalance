{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9f6fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from smooth_epidata import *\n",
    "\n",
    "sel_url = 'https://raw.githubusercontent.com/biobotanalytics/covid19-wastewater-data/master/wastewater_by_county.csv'\n",
    "ww_data = pd.read_csv(sel_url)\n",
    "\n",
    "abvs = pd.read_csv('us_states_abbr_list.txt', header=None)[0].to_list()\n",
    "\n",
    "\n",
    "fips_tab = pd.read_csv('reich_fips.txt')\n",
    "\n",
    "# %Change US code to 0\n",
    "def impute_us(x):\n",
    "    if x=='US':\n",
    "        return '0'\n",
    "    else:\n",
    "        return x\n",
    "fips_tab['location'] = fips_tab['location'].apply(impute_us)\n",
    "\n",
    "ww_data['fipscode'] = ww_data.fipscode.astype(str)\n",
    "\n",
    "ww_data_fips = ww_data.fipscode.to_list()\n",
    "\n",
    "fips_tab_codes = fips_tab['location'].to_list()\n",
    "\n",
    "fips_idx = []\n",
    "second_grp = [float(k) for k in (fips_tab_codes)]\n",
    "for w in range(len(ww_data_fips)):\n",
    "    if float(ww_data_fips[w]) in second_grp:\n",
    "        \n",
    "        fips_idx.append(second_grp.index(float(ww_data_fips[w])))\n",
    "    else:\n",
    "        fips_idx.append(0)\n",
    "        \n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "xx = ww_data.sampling_week.to_list()     \n",
    "### %sampling_week column values\n",
    "\n",
    "\n",
    "# %Set whichday to be array of day # correspodning to day since 2020,1,23\n",
    "\n",
    "whichday = np.zeros(len(xx))\n",
    "for ii in range(len(xx)):\n",
    "    whichday[ii] = int((datetime.strptime(xx[ii], '%Y-%m-%d') - datetime(2020, 1, 23)).days)\n",
    "\n",
    "# THIS SHOULD BE UPDATED CONSISTENTLY. SHOULD BE ON GITHUB BUT NOT BEING UPDATED: https://github.com/scc-usc/ReCOVER-COVID-19/tree/master/matlab%20scripts. FOR NOW CAN GET FROM SERVER IF NEED BE\n",
    "# %%\n",
    "from latest_us_data import *\n",
    "get_data()\n",
    "filehandler = open(\"data_4.pkl\", 'rb') \n",
    "#     print(a)\n",
    "data_4 = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "# %Define 2 matrices of size = (# of states x days)\n",
    "\n",
    "ww_ts = np.zeros((data_4.shape[0],data_4.shape[1]))\n",
    "\n",
    "ww_pres = np.zeros((data_4.shape[0],data_4.shape[1]))\n",
    "\n",
    "#  %Get inicies of states according to abvs\n",
    "    \n",
    "abvs_idx = [abvs.index(i) for i in ww_data.state if i in abvs]\n",
    "\n",
    "popu = np.loadtxt('us_states_population_data.txt')\n",
    "\n",
    "\n",
    "#  %ignore dates before 2020,1,23 and unavailable states\n",
    "# # %Set to 1 if data available for state/day\n",
    "#     % fill other matrix with effective_concentration_rolling_average*(pop\n",
    "#     % of county / pop of state)\n",
    "\n",
    "for ii in range(len(xx)):\n",
    "    if whichday[ii] < 1 or fips_idx[ii] < 1:\n",
    "        continue\n",
    "#     print(ii)\n",
    "    ww_pres[abvs_idx[ii], int(whichday[ii])] = 1\n",
    "    ww_ts[abvs_idx[ii], int(whichday[ii])] = ww_ts[abvs_idx[ii], int(whichday[ii])] + ww_data.effective_concentration_rolling_average[ii]*fips_tab.population[fips_idx[ii]]/popu[abvs_idx[ii]]\n",
    "\n",
    "#  %Set any ww_ts data to nan if no data available in ww_pres\n",
    "\n",
    "ww_ts[ww_pres < 1] = np.nan\n",
    "\n",
    "# %Set any ww_ts data to nan if <=0\n",
    "\n",
    "ww_ts[ww_ts <= 0] = np.nan\n",
    "\n",
    "# %Fill missing values with linear interpolation with a moving mean of winodw size = 2 and without filling endpoints\n",
    "\n",
    "ww_ts1 = pd.DataFrame(ww_ts).interpolate( limit_direction='both', axis=1).to_numpy()\n",
    "\n",
    "# %use movingmean filter to smooth\n",
    "\n",
    "ww_tsm = pd.DataFrame(ww_ts1).rolling(14, axis=1).mean().to_numpy()\n",
    "\n",
    "#  %Use custom smooth_epidata to filter . Smooth_epidata function definition in other file \n",
    "data_4_s = Smooth_epiData(data_4,smooth_factor = 14,week_correction = 0,week_smoothing = 1)\n",
    "\n",
    "\n",
    "# %Let day 0=0s and instead of having cumilative cases up to day, use diff to find daily cases at day x\n",
    "# %%\n",
    "\n",
    "\n",
    "data_diff = np.insert(np.diff(data_4_s, axis=1), 0, 0, axis=1)\n",
    "\n",
    "# %I believe the first starts at 6 and the second at 3 to make the weeks both\n",
    "# %start on Sunday (This is basically hard coded\n",
    "\n",
    "\n",
    "# %ww_ts is also cumilative data, so making first entries 0 & ignoring first couple entries & then skipping each 7 afterwards makes this weekly values\n",
    "\n",
    "ww_tsw = np.insert(ww_ts[:, 5::7], 0, 0, axis=1)\n",
    "\n",
    "#  %Also make this weekly data as above\n",
    "\n",
    "weekly_dat = np.insert(np.diff(ww_tsw, axis=1), 0, 0, axis=1)\n",
    "\n",
    "\n",
    "import csaps\n",
    "\n",
    "wlag = 7\n",
    "ww_tsm[ww_ts == 0] = np.nan\n",
    "f = np.empty_like(ww_ts)\n",
    "f[:, wlag:] = ww_ts[:, :-wlag] / data_diff[:, wlag:]\n",
    "f[:, :200] = np.nan\n",
    "f1 = f\n",
    "\n",
    "\n",
    "for jj in range(f1.shape[0]):\n",
    "    xx = np.flatnonzero(~np.isnan(f1[jj,:]))\n",
    "    xx2 = np.flatnonzero(np.isfinite(f1[jj,:]))\n",
    "    xx = np.array(list(set(xx2).intersection(set(xx))))\n",
    "    xx.sort()\n",
    "    \n",
    "    \n",
    "    if (len(xx) > 5) and (sum((xx>200) & (xx<600))) > 2:\n",
    "        yy = f1[jj, xx]\n",
    "        f[jj, 200:] = sp = csaps.CubicSmoothingSpline(xx, yy,smooth = 0.0001)(list(range(200,f.shape[1])))\n",
    "    f[jj,:] = pd.DataFrame(f[jj,:]).interpolate( limit_direction='both', axis=1).to_numpy().ravel()\n",
    "    f[jj,:] = pd.DataFrame(f[jj,:]).interpolate(limit_direction='both', axis=1).to_numpy().ravel()\n",
    "\n",
    "\n",
    "f[f<0] = 0\n",
    "f = pd.DataFrame(f).rolling(7,axis=1).mean().to_numpy()\n",
    "ww_adj = pd.DataFrame(data_diff*f).rolling(14,axis=1).mean().to_numpy()\n",
    "\n",
    "# %Run CDC_sero script and save variables to workspace for use on this script\n",
    "\n",
    "from CDC_Sero import *\n",
    "CDC_SERO_Function()\n",
    "\n",
    "\n",
    "filehandler = open(\"true_new_infec.pkl\", 'rb') \n",
    "true_new_infec = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "filehandler = open(\"un_array.pkl\", 'rb') \n",
    "un_array = pickle.load(filehandler)\n",
    "\n",
    "\n",
    "true_new_infec[0] = (true_new_infec[0]+abs(true_new_infec[0]))/2\n",
    "true_new_infec[1] = (true_new_infec[1]+abs(true_new_infec[1]))/2\n",
    "true_new_infec[2] = (true_new_infec[2]+abs(true_new_infec[2]))/2\n",
    "\n",
    "\n",
    "eq_range = range(200,601)\n",
    "nz_idx = ww_adj[:, eq_range]>0.5\n",
    "\n",
    "ww_year1 = np.nansum(ww_adj[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "# %For each year, find some of none missing data as above and divide it by ww\n",
    "# %then multiply by ww_adj before smoothing\n",
    "\n",
    "sero_year1 = np.nansum(true_new_infec[0].iloc[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "\n",
    "true_new_infec_ww = [0,0,0]\n",
    "b = sero_year1/ww_year1\n",
    "true_new_infec_ww[0] = ww_adj*b.reshape((b.size, 1))\n",
    "\n",
    "\n",
    "true_new_infec_ww[0][np.isnan(true_new_infec_ww[0])] = 0\n",
    "\n",
    "\n",
    "true_new_infec_ww[0] = pd.DataFrame(np.maximum(true_new_infec_ww[0],true_new_infec[0].to_numpy())).rolling(14,axis=1,min_periods=0).mean().to_numpy()\n",
    "\n",
    "\n",
    "eq_range = range(200,601)\n",
    "nz_idx = ww_adj[:, eq_range]>0.5\n",
    "\n",
    "# ww_year2 = np.nansum(ww_adj[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "\n",
    "sero_year2 = np.nansum(true_new_infec[1].iloc[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "b = sero_year2/ww_year1\n",
    "true_new_infec_ww[1] = ww_adj*b.reshape((b.size, 1))\n",
    "\n",
    "\n",
    "true_new_infec_ww[1][np.isnan(true_new_infec_ww[1])] = 0\n",
    "\n",
    "\n",
    "true_new_infec_ww[1] = pd.DataFrame(np.maximum(true_new_infec_ww[1],true_new_infec[1].to_numpy())).rolling(14,axis=1,min_periods=0).mean().to_numpy()\n",
    "\n",
    "\n",
    "eq_range = range(200,601)\n",
    "nz_idx = ww_adj[:, eq_range]>0.5\n",
    "\n",
    "# ww_year2 = np.nansum(ww_adj[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "\n",
    "sero_year3 = np.nansum(true_new_infec[2].iloc[:, eq_range]*nz_idx, axis=1)\n",
    "\n",
    "b = sero_year3/ww_year1\n",
    "true_new_infec_ww[2] = ww_adj*b.reshape((b.size, 1))\n",
    "\n",
    "\n",
    "true_new_infec_ww[2][np.isnan(true_new_infec_ww[2])] = 0\n",
    "\n",
    "\n",
    "true_new_infec_ww[2] = pd.DataFrame(np.maximum(true_new_infec_ww[2],true_new_infec[2].to_numpy())).rolling(14,axis=1,min_periods=0).mean().to_numpy()\n",
    "\n",
    "# %replace bad states\n",
    "\n",
    "\n",
    "bad_states = ((~np.isnan(ww_ts)).sum(axis=1) < 1) & ((~np.isnan(true_new_infec[1])).sum(axis=1) > 1)\n",
    "\n",
    "\n",
    "true_new_infec_ww[0][bad_states, :] = true_new_infec[0][bad_states].to_numpy()\n",
    "true_new_infec_ww[1][bad_states, :] = true_new_infec[1][bad_states].to_numpy()\n",
    "true_new_infec_ww[2][bad_states, :] = true_new_infec[2][bad_states].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "bad_states = (np.isnan(ww_ts).sum(axis=1)<1) & (np.isnan(true_new_infec[1]).sum(axis=1)<1)\n",
    "\n",
    "\n",
    "# true_new_infec_ww[0][bad_states, :] = true_new_infec[0][bad_states, :]\n",
    "# true_new_infec_ww[1][bad_states, :] = true_new_infec[1][bad_states, :]\n",
    "# true_new_infec_ww[2][bad_states, :] = true_new_infec[2][bad_states, :]\n",
    "\n",
    "true_new_infec_ww[0][bad_states, :] = data_diff[bad_states, :]\n",
    "true_new_infec_ww[1][bad_states, :] = data_diff[bad_states, :]\n",
    "true_new_infec_ww[2][bad_states, :] = data_diff[bad_states, :]\n",
    "\n",
    "import pickle \n",
    "with open(\"true_new_infec_ww.pkl\", \"wb\") as f:\n",
    "    pickle.dump(true_new_infec_ww, f)\n",
    "with open(\"true_new_infec_final.pkl\", \"wb\") as f:\n",
    "    pickle.dump(true_new_infec, f)\n",
    "\n",
    "\n",
    "print(\"Process completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filehandler = open(\"un_array.pkl\", 'rb') \n",
    "# un_array = pickle.load(filehandler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
